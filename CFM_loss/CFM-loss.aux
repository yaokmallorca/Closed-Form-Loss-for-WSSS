\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{IEEEexample:BSTcontrol}
\citation{chen2017deeplab,hariharan2015hypercolumns,chen2017rethinking,long2015fully}
\citation{zhang2019survey}
\citation{zhou2016learning}
\citation{huang2018weakly,wei2018revisiting,wang2020self}
\citation{everingham2012pascal}
\citation{cordts2016cityscapes}
\citation{levin2007closed}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\newlabel{intro}{{I}{1}{Introduction}{section.1}{}}
\citation{kolesnikov2016seed}
\citation{zhou2016learning}
\citation{huang2018weakly}
\citation{ahn2019weakly}
\citation{ma2019affinitynet}
\citation{choe2019attention}
\citation{wei2018revisiting}
\citation{khoreva2017simple}
\citation{song2019box}
\citation{krahenbuhl2011efficient}
\citation{hsu2019weakly}
\citation{boykov2001interactive}
\citation{grady2006random}
\citation{bai2009geodesic}
\citation{tang2018regularized}
\citation{wang2019boundary}
\citation{yao2021weakly}
\citation{oktay2018attention}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\newlabel{related}{{II}{2}{Related Work}{section.2}{}}
\citation{levin2007closed}
\citation{chen2018encoder}
\citation{ronneberger2015u}
\citation{romera2017erfnet}
\citation{levin2007closed}
\citation{levin2007closed}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{3}{section.3}}
\newlabel{method}{{III}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Background}{3}{subsection.3.1}}
\newlabel{subsec:background}{{\unhbox \voidb@x \hbox {III-A}}{3}{Background}{subsection.3.1}{}}
\newlabel{func_Ii}{{1}{3}{Background}{equation.3.1}{}}
\newlabel{func_alphai}{{2}{3}{Background}{equation.3.2}{}}
\newlabel{func_alphai_2}{{3}{3}{Background}{equation.3.3}{}}
\newlabel{func_j_alpha_a_b}{{4}{3}{Background}{equation.3.4}{}}
\newlabel{func_quadratic_cost}{{5}{3}{Background}{equation.3.5}{}}
\newlabel{laplacian_entry}{{6}{3}{Background}{equation.3.6}{}}
\newlabel{Kronecker delta}{{7}{3}{Background}{equation.3.7}{}}
\newlabel{laplacian_entry_bgr}{{8}{3}{Background}{equation.3.8}{}}
\newlabel{optimize_alpha}{{9}{3}{Background}{equation.3.9}{}}
\newlabel{sparse_linear_system}{{10}{3}{Background}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Closed Form Matting Loss Function}{4}{subsection.3.2}}
\newlabel{subsec:closed-form-loss}{{\unhbox \voidb@x \hbox {III-B}}{4}{Closed Form Matting Loss Function}{subsection.3.2}{}}
\newlabel{mseloss_closed_form}{{11}{4}{Closed Form Matting Loss Function}{equation.3.11}{}}
\newlabel{closed_form_loss_one_channel}{{12}{4}{Closed Form Matting Loss Function}{equation.3.12}{}}
\newlabel{closed_form_loss_mul_channel}{{13}{4}{Closed Form Matting Loss Function}{equation.3.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Scribble Annotations}{4}{subsection.3.3}}
\newlabel{subsec:weak-annotations}{{\unhbox \voidb@x \hbox {III-C}}{4}{Scribble Annotations}{subsection.3.3}{}}
\newlabel{scribble_define_eq}{{14}{4}{Scribble Annotations}{equation.3.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Samples of weak annotations, from more to less informative: (a) and (b) are samples from the inspection tasks, and only black and white scribbles are used due to the binary segmentation task; (c) and (d) come form the MCSS tasks, and different color scribbles are used to distinguish the category of target.}}{4}{figure.1}}
\newlabel{fg:weak-annotations}{{1}{4}{Samples of weak annotations, from more to less informative: (a) and (b) are samples from the inspection tasks, and only black and white scribbles are used due to the binary segmentation task; (c) and (d) come form the MCSS tasks, and different color scribbles are used to distinguish the category of target}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Multi-Label Classification}{4}{subsection.3.4}}
\newlabel{subsec:mlc_model}{{\unhbox \voidb@x \hbox {III-D}}{4}{Multi-Label Classification}{subsection.3.4}{}}
\newlabel{loss_multi_class}{{15}{4}{Multi-Label Classification}{equation.3.15}{}}
\citation{chen2018encoder}
\citation{chen2017rethinking}
\citation{papandreou2015modeling,chen2017deeplab}
\citation{chollet2017xception}
\citation{chen2017rethinking}
\citation{chen2017deeplab}
\citation{lazebnik2006beyond,he2015spatial}
\citation{hariharan2015hypercolumns}
\citation{chen2017rethinking}
\citation{chollet2017xception}
\citation{qi2017deformable}
\citation{chen2018encoder}
\citation{lin2014microsoft}
\citation{everingham2012pascal}
\citation{qi2017deformable}
\citation{ronneberger2015u}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Segmentation Backbones}{5}{subsection.3.5}}
\newlabel{subsec:seg_backbone}{{\unhbox \voidb@x \hbox {III-E}}{5}{Segmentation Backbones}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-E}1}DeepLabV3+}{5}{subsubsection.3.5.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {III-E}1a}Atrous Convolution}{5}{paragraph.3.5.1.1}}
\newlabel{atrous_conv}{{16}{5}{Atrous Convolution}{equation.3.16}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {III-E}1b}Depthwise Separable Convolution}{5}{paragraph.3.5.1.2}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {III-E}1c}Atrous Spatial Pyramid Pooling}{5}{paragraph.3.5.1.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {III-E}1d}Encoder-Decoder Architecture}{5}{paragraph.3.5.1.4}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {III-E}1e}Modified Aligned Xception}{5}{paragraph.3.5.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-E}2}U-Net}{5}{subsubsection.3.5.2}}
\citation{romera2017erfnet}
\citation{he2016deep}
\citation{he2016deep,zagoruyko2016wide,rastegari2016xnor}
\citation{he2016deep}
\citation{romera2017erfnet}
\citation{he2016deep}
\citation{romera2017erfnet}
\citation{paszke2016enet}
\citation{yu2015multi}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training schematic diagram of DeepLabV3+ with the closed form loss. The encoder model is employed to extract multi-scale contextual information by using atrous convolution, then the ASPP model is used to integrate multi-scale information. A simple but effective decoder model is proposed to refine the segmentation results. The MLC model is connected to the encoder, which is used to obtain the category information of targets in the image. In the end, our CFM loss is used to train the network. }}{6}{figure.2}}
\newlabel{fg:closed-form-loss-trianing}{{2}{6}{Training schematic diagram of DeepLabV3+ with the closed form loss. The encoder model is employed to extract multi-scale contextual information by using atrous convolution, then the ASPP model is used to integrate multi-scale information. A simple but effective decoder model is proposed to refine the segmentation results. The MLC model is connected to the encoder, which is used to obtain the category information of targets in the image. In the end, our CFM loss is used to train the network}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A diagrams illustration of the atrous convolution. In this figure. only 9 red points for a $3\times 3$ kernel are involved in the convolution operation, and other points in the receptive field are ignored. A is the standard convolution, where the $r$ is 1, its receptive field is only $3\times 3$; the $r$ in B is 2, and its receptive field is $7\times 7$; while C is the 4-dilated convolution, and its receptive field is $15\times 15$.}}{6}{figure.3}}
\newlabel{fg:atrous_conv}{{3}{6}{A diagrams illustration of the atrous convolution. In this figure. only 9 red points for a $3\times 3$ kernel are involved in the convolution operation, and other points in the receptive field are ignored. A is the standard convolution, where the $r$ is 1, its receptive field is only $3\times 3$; the $r$ in B is 2, and its receptive field is $7\times 7$; while C is the 4-dilated convolution, and its receptive field is $15\times 15$}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-E}3}ERFNet}{6}{subsubsection.3.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments and Discussion}{6}{section.4}}
\newlabel{exp_dis}{{IV}{6}{Experiments and Discussion}{section.4}{}}
\citation{russakovsky2015imagenet}
\citation{levin2007closed}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The architecture of U-Net. In this figure, \textit  {Conv} represents convolutional layer with $3\times 3$ kernel; \textit  {Conv\nobreakspace  {}\&\nobreakspace  {}ReLU} indicates convolutional layer connected with \textit  {ReLU} activation; \textit  {Pool} layer is used to decrease the dimension of the input feature; \textit  {Upsample} is used to increase the dimension of the input feature; \textit  {Softmax} activated function is used to get the segmentation for every category. }}{7}{figure.4}}
\newlabel{fg:unet}{{4}{7}{The architecture of U-Net. In this figure, \textit {Conv} represents convolutional layer with $3\times 3$ kernel; \textit {Conv~\&~ReLU} indicates convolutional layer connected with \textit {ReLU} activation; \textit {Pool} layer is used to decrease the dimension of the input feature; \textit {Upsample} is used to increase the dimension of the input feature; \textit {Softmax} activated function is used to get the segmentation for every category}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The illustration of (a) Non-bottleneck in \cite  {he2016deep} and (b) Non-bottleneck-1D in \cite  {romera2017erfnet}. In this figure, $C$ represents the channels of feature map. In the convolution block, ($d_{1} \times d_{2},\nobreakspace  {}c$) indicates the kernel size ($d_1, d_2$) and the output channels ($c$) of feature maps. \textit  {EltSum} denotes element-wise sum. }}{7}{figure.5}}
\newlabel{fg:bottleneck}{{5}{7}{The illustration of (a) Non-bottleneck in \cite {he2016deep} and (b) Non-bottleneck-1D in \cite {romera2017erfnet}. In this figure, $C$ represents the channels of feature map. In the convolution block, ($d_{1} \times d_{2},~c$) indicates the kernel size ($d_1, d_2$) and the output channels ($c$) of feature maps. \textit {EltSum} denotes element-wise sum}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Experiment Settings}{7}{subsection.4.1}}
\newlabel{sec:exp_setup}{{\unhbox \voidb@x \hbox {IV-A}}{7}{Experiment Settings}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {IV-A}0a}Datasets}{7}{paragraph.4.1.0.1}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces A detailed demonstration for the architecture of ERFNet. \textit  {OUT-C} represents the output channels, \textit  {OUT-DIM} indicates the dimension of output feature maps, and \textit  {C} denotes the number of category. }}{7}{table.1}}
\newlabel{tab:erf_arch}{{I}{7}{A detailed demonstration for the architecture of ERFNet. \textit {OUT-C} represents the output channels, \textit {OUT-DIM} indicates the dimension of output feature maps, and \textit {C} denotes the number of category}{table.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {IV-A}0b}Implementation Details}{7}{paragraph.4.1.0.2}}
\citation{chen2018encoder}
\citation{ronneberger2015u}
\citation{romera2017erfnet}
\citation{cordts2016cityscapes}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\unhbox \voidb@x \hbox {IV-A}0c}Evaluation Metrics}{8}{paragraph.4.1.0.3}}
\newlabel{eq:metric_sad}{{17}{8}{Evaluation Metrics}{equation.4.17}{}}
\newlabel{eq:metric_mse}{{18}{8}{Evaluation Metrics}{equation.4.18}{}}
\newlabel{eq:metric_ge}{{19}{8}{Evaluation Metrics}{equation.4.19}{}}
\newlabel{eq:obtain_seg}{{20}{8}{Evaluation Metrics}{equation.4.20}{}}
\newlabel{eq:metric_miou}{{21}{8}{Evaluation Metrics}{equation.4.21}{}}
\newlabel{eq:metric_rec_prev}{{22}{8}{Evaluation Metrics}{equation.4.22}{}}
\newlabel{eq:metric_f1}{{23}{8}{Evaluation Metrics}{equation.4.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Evaluation for Matting Performance}{8}{subsection.4.2}}
\newlabel{sec:matting_eval_ins}{{\unhbox \voidb@x \hbox {IV-B}}{8}{Evaluation for Matting Performance}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The heat map of matting results: the first row shows the example from the BIO task, and the second row represents the example form COR task; the first column shows the input image, the matting results from CFM algorithm is shown in the second column; from the 3- to 7-th rows, the matting results from Res101-, Xception-, MobileNet-based DeepLabV3+ network, U-Net, and ERFNet are shown. }}{9}{figure.6}}
\newlabel{fg:compare_matting_figure}{{6}{9}{The heat map of matting results: the first row shows the example from the BIO task, and the second row represents the example form COR task; the first column shows the input image, the matting results from CFM algorithm is shown in the second column; from the 3- to 7-th rows, the matting results from Res101-, Xception-, MobileNet-based DeepLabV3+ network, U-Net, and ERFNet are shown}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Examples of matting results and ground truth of the COR task.}}{9}{figure.7}}
\newlabel{fg:comp_cor_matte_results}{{7}{9}{Examples of matting results and ground truth of the COR task}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Segmentation Results of Inspection Tasks}{9}{subsection.4.3}}
\newlabel{sec:seg_performance_ins}{{\unhbox \voidb@x \hbox {IV-C}}{9}{Segmentation Results of Inspection Tasks}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Segmentation Results of MCSS Tasks}{9}{subsection.4.4}}
\newlabel{sec:seg_performance_mcss}{{\unhbox \voidb@x \hbox {IV-D}}{9}{Segmentation Results of MCSS Tasks}{subsection.4.4}{}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,Bibliography}
\bibcite{chen2017deeplab}{1}
\bibcite{hariharan2015hypercolumns}{2}
\bibcite{chen2017rethinking}{3}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Metrics for the different experiments performed, varying the segmentation network. MSE stands for the mean square error, mAD indicates the mean sum of absolute difference, and GE represents the Gradient Error.}}{10}{table.2}}
\newlabel{tab:compare_matting_table}{{II}{10}{Metrics for the different experiments performed, varying the segmentation network. MSE stands for the mean square error, mAD indicates the mean sum of absolute difference, and GE represents the Gradient Error}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-E}}Experimental Results Visualization}{10}{subsection.4.5}}
\newlabel{sec:vis_exp_results}{{\unhbox \voidb@x \hbox {IV-E}}{10}{Experimental Results Visualization}{subsection.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{10}{section.5}}
\newlabel{conclusion}{{V}{10}{Conclusion}{section.5}{}}
\@writefile{toc}{\contentsline {section}{References}{10}{section*.1}}
\bibcite{long2015fully}{4}
\bibcite{zhang2019survey}{5}
\bibcite{zhou2016learning}{6}
\bibcite{huang2018weakly}{7}
\bibcite{wei2018revisiting}{8}
\bibcite{wang2020self}{9}
\bibcite{everingham2012pascal}{10}
\bibcite{cordts2016cityscapes}{11}
\bibcite{levin2007closed}{12}
\bibcite{kolesnikov2016seed}{13}
\bibcite{ahn2019weakly}{14}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces List of segmentation metrics on the validation set for the two inspection tasks. \textit  {Network(backbone)} refers the segmentation network involved in our solution; \textit  {\#TH} stands for the threshold to get the segmentation result based on the matting results; \textit  {mIOU} represents the mean intersection over union; \textit  {Rec}, \textit  {Prec}, and \textit  {F1} indicates the recall, precision, and F1 respectively. Best performance is highlighted in bold. }}{11}{table.3}}
\newlabel{tab:compare_seg_table}{{III}{11}{List of segmentation metrics on the validation set for the two inspection tasks. \textit {Network(backbone)} refers the segmentation network involved in our solution; \textit {\#TH} stands for the threshold to get the segmentation result based on the matting results; \textit {mIOU} represents the mean intersection over union; \textit {Rec}, \textit {Prec}, and \textit {F1} indicates the recall, precision, and F1 respectively. Best performance is highlighted in bold}{table.3}{}}
\bibcite{ma2019affinitynet}{15}
\bibcite{choe2019attention}{16}
\bibcite{khoreva2017simple}{17}
\bibcite{song2019box}{18}
\bibcite{krahenbuhl2011efficient}{19}
\bibcite{hsu2019weakly}{20}
\bibcite{boykov2001interactive}{21}
\bibcite{grady2006random}{22}
\bibcite{bai2009geodesic}{23}
\bibcite{tang2018regularized}{24}
\bibcite{wang2019boundary}{25}
\bibcite{yao2021weakly}{26}
\bibcite{oktay2018attention}{27}
\bibcite{chen2018encoder}{28}
\bibcite{ronneberger2015u}{29}
\bibcite{romera2017erfnet}{30}
\bibcite{papandreou2015modeling}{31}
\bibcite{chollet2017xception}{32}
\bibcite{lazebnik2006beyond}{33}
\bibcite{he2015spatial}{34}
\bibcite{qi2017deformable}{35}
\bibcite{lin2014microsoft}{36}
\bibcite{he2016deep}{37}
\bibcite{zagoruyko2016wide}{38}
\bibcite{rastegari2016xnor}{39}
\bibcite{paszke2016enet}{40}
\bibcite{yu2015multi}{41}
\bibcite{russakovsky2015imagenet}{42}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Per-class IOU on validation set of PASCAL VOC 2012 subset.}}{12}{table.4}}
\newlabel{tab:tab_ablation_voc}{{IV}{12}{Per-class IOU on validation set of PASCAL VOC 2012 subset}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Per-class IOU on the validation set of CityScape subset.}}{12}{table.5}}
\newlabel{tab:tab_ablation_cityscape}{{V}{12}{Per-class IOU on the validation set of CityScape subset}{table.5}{}}
\citation{levin2007closed}
\citation{levin2007closed}
\@writefile{toc}{\contentsline {section}{\numberline {I}The Optimized Target}{13}{section.1}}
\newlabel{target_derivation}{{I}{13}{The Optimized Target}{section.1}{}}
\newlabel{equ1}{{24}{13}{The Optimized Target}{equation.1.24}{}}
\newlabel{equ2}{{25}{13}{The Optimized Target}{equation.1.25}{}}
\newlabel{equ3}{{26}{13}{The Optimized Target}{equation.1.26}{}}
\newlabel{equ4}{{27}{13}{The Optimized Target}{equation.1.27}{}}
\newlabel{equ5}{{28}{13}{The Optimized Target}{equation.1.28}{}}
\newlabel{equ6}{{29}{13}{The Optimized Target}{equation.1.29}{}}
\newlabel{equ7}{{30}{13}{The Optimized Target}{equation.1.30}{}}
\newlabel{equ8}{{31}{13}{The Optimized Target}{equation.1.31}{}}
\newlabel{equ9}{{32}{13}{The Optimized Target}{equation.1.32}{}}
\newlabel{equ10}{{33}{13}{The Optimized Target}{equation.1.33}{}}
\newlabel{equ11}{{34}{13}{The Optimized Target}{equation.1.34}{}}
\newlabel{equ12}{{35}{13}{The Optimized Target}{equation.1.35}{}}
\newlabel{equ13}{{36}{13}{The Optimized Target}{equation.1.36}{}}
\newlabel{equ14}{{37}{13}{The Optimized Target}{equation.1.37}{}}
\newlabel{equ15}{{38}{13}{The Optimized Target}{equation.1.38}{}}
\newlabel{equ16}{{39}{13}{The Optimized Target}{equation.1.39}{}}
\newlabel{equ17}{{40}{14}{The Optimized Target}{equation.1.40}{}}
\newlabel{equ18}{{41}{14}{The Optimized Target}{equation.1.41}{}}
\newlabel{equ19}{{42}{14}{The Optimized Target}{equation.1.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}The Matting Laplacian Matrix}{14}{section.2}}
\newlabel{laplacian_derivation}{{II}{14}{The Matting Laplacian Matrix}{section.2}{}}
\newlabel{equ20}{{43}{14}{The Matting Laplacian Matrix}{equation.2.43}{}}
\newlabel{equ21}{{44}{14}{The Matting Laplacian Matrix}{equation.2.44}{}}
\newlabel{equ22}{{45}{14}{The Matting Laplacian Matrix}{equation.2.45}{}}
\newlabel{equ23}{{46}{14}{The Matting Laplacian Matrix}{equation.2.46}{}}
\newlabel{equ24}{{47}{14}{The Matting Laplacian Matrix}{equation.2.47}{}}
\newlabel{equ25}{{48}{15}{The Matting Laplacian Matrix}{equation.2.48}{}}
\newlabel{equ26}{{49}{15}{The Matting Laplacian Matrix}{equation.2.49}{}}
\newlabel{equ27}{{50}{15}{The Matting Laplacian Matrix}{equation.2.50}{}}
\newlabel{equ28}{{51}{15}{The Matting Laplacian Matrix}{equation.2.51}{}}
\newlabel{equ29}{{52}{15}{The Matting Laplacian Matrix}{equation.2.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Examples of segmentation and matting results for the BIO task. For every sample, the first image in the 1st row is the input image; and the 2nd image is the scribble annotation; from the 3- to 7-th, the images are the segmentation results using different segmentation network; in the second row, the first two images are the segmentation ground truth and matting results of the CFM algorithm; the rest images in the second row are the matting results of our approach by using different networks.}}{16}{figure.8}}
\newlabel{fg:vis_bio_results}{{8}{16}{Examples of segmentation and matting results for the BIO task. For every sample, the first image in the 1st row is the input image; and the 2nd image is the scribble annotation; from the 3- to 7-th, the images are the segmentation results using different segmentation network; in the second row, the first two images are the segmentation ground truth and matting results of the CFM algorithm; the rest images in the second row are the matting results of our approach by using different networks}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Examples of segmentation and matting results for the COR task. For every sample, the first image in the 1st row is the input image; and the 2nd image is the scribble annotation; from the 3- to 7-th, the images are the segmentation results using different segmentation network; in the second row, the first two images are the segmentation ground truth and matting results of the CFM algorithm; the rest images in the second row are the matting results of our approach by using different networks.}}{17}{figure.9}}
\newlabel{fg:vis_corrosion_results}{{9}{17}{Examples of segmentation and matting results for the COR task. For every sample, the first image in the 1st row is the input image; and the 2nd image is the scribble annotation; from the 3- to 7-th, the images are the segmentation results using different segmentation network; in the second row, the first two images are the segmentation ground truth and matting results of the CFM algorithm; the rest images in the second row are the matting results of our approach by using different networks}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Examples of segmentation and matting results for the VOC task. For every example, from left to right, the segmentation result is shown firstly; secondly, the scribble annotation is shown; the rest of images are the matting results of different categories. }}{18}{figure.10}}
\newlabel{fg:vis_voc_results}{{10}{18}{Examples of segmentation and matting results for the VOC task. For every example, from left to right, the segmentation result is shown firstly; secondly, the scribble annotation is shown; the rest of images are the matting results of different categories}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Examples of segmentation and matting results for the CityScape task. For every example, from left to right, the segmentation result is shown firstly; then, the matting results of different categories are shown. }}{19}{figure.11}}
\newlabel{fg:vis_cityscape_results}{{11}{19}{Examples of segmentation and matting results for the CityScape task. For every example, from left to right, the segmentation result is shown firstly; then, the matting results of different categories are shown}{figure.11}{}}
